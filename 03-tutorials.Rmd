# Tutorials 

```{r include=FALSE}
library(tidyverse)
library(rvest)
```


## Writing functions in R 




## Working with different datatypes (and how to convert between them)




## Regular expressions in R 




## Scraping data from HTML 

This tutorial is adapted from [Chris Bail's Screen-scraping in R](https://cbail.github.io/textasdata/screenscraping/rmarkdown/Screenscraping_in_R.html) & [Wickham et al.'s R for Data Science](https://r4ds.hadley.nz/webscraping). 

We'll be using the package [rvest](https://rvest.tidyverse.org/index.html) to scrape information from html pages, which is useful for information contained within tables such as this [Interjection dictionary](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html) or the top movies on [IMDB](https://www.imdb.com/chart/top/).

Let's start with the table in [Interjection dictionary](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html). 

First, we'll read the entire html source code from the website we are interest into R. 

```{r}
interjections_html <- read_html("https://www.vidarholen.net/contents/interjections/") # Reading the entire page into R 
interjections_html
```

The result is html code, which is the programming language web developers use to define the structure and content of the website. In Google, you can inspect the html code of any website by right clicking with your cursor and pressing the "inspect" tab. Importantly, HTML code has a nested structure, typically including "head" a "body" sections of the webpage. The content are mostly contained within tags such as "p" (paragraph) and "h1" (heading 1), I won't go into too much detail what these are but feel free to look up any HTML tutorial to learn more about these elements.

```{r echo=FALSE, out.width = '800px', fig.align='center'}
knitr::include_graphics('./assets/html_inspect_screenshot.png')
```

Once you have the html code in R, you can use the html_nodes() function to specify what sort of elements you want to extract. For our purpose, we will specify "table":  

```{r}
interjections_dict1 <- interjections_html %>% 
  html_nodes("table") %>%
  .[[1]] %>% # Which table you want to extract (if there are multiple tables on a webpage)
  html_table()

head(interjections_dict1) # This gives you a dataframe that you can work with! 
```

Alternatively, you can specify an xpath to directly call the specific element you want to turn into a dataframe. You can find the xpath of an element by hovering over the HTML code that highlights the part of the website you want. For instance: 

```{r echo=FALSE, out.width = '800px', fig.align='center'}
knitr::include_graphics('./assets/html_xpath_screenshot.png')
```

Learn more about xpaths [here](https://www.w3schools.com/xml/xpath_syntax.asp).

```{r}
interjections_dict2 <- html_node(interjections_html, xpath = '//*[@id="it"]') %>% 
  html_table()
head(interjections_dict2)
```

### What if you wanted other information contained within a page? 

You can scrape specific elements of a webpage, depending on the HTML element you specify. Let's say you want to scrape all the text within paragraphs ("p") of [Sigmund Freud's Wikipedia](https://en.wikipedia.org/wiki/Sigmund_Freud) website. 

```{r}
freud_html <- read_html("https://en.wikipedia.org/wiki/Sigmund_Freud") %>% # Reading the entire wiki page into R 
  html_elements("p") %>% # Specify all "p" elements in the webpage
  html_text2() # Extract the text content of the HTML elements 

freud_html[[3]] # This gives you a list of all the paragraph text on the webpage
```

### Some caveats 

<b> Legality </b>

If the data is public, non-personal, and factual, it should be fine to scrape*. You should cite the websites you use, and always read the terms of conditions if present. 

<b> APIs </b> 

This short tutorial doesn't cover more dynamic websites, or websites that utilise an Application Programming Interface (API), which are a set of functions or procedures the website has specified to access their data. 
